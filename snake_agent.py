{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\nimport numpy as np\nfrom PIL import Image\nfrom collections import deque,namedtuple\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as T\n\nBATCH_SIZE = 100\nGAMMA = 0.99\nn_actions = 4\nMEMORY_SIZE = 100000\n\ndef get_screen(settings,snake,apple,device):\n    \n    screen = torch.zeros(100)-1\n    screen[0] = apple.x\n    screen[1] = apple.y\n        \n    for index,i in enumerate(snake.body):\n        screen[2*index+2] = i[0]\n        screen[2*index+3] = i[1]\n        if index==48:\n            break\n    \n    #Resize, and add a batch dimension (BCHW)\n    #resize = T.Compose([T.ToPILImage(),T.Resize((30,30), interpolation=T.InterpolationMode.BICUBIC),T.ToTensor()])\n    #screen = resize(screen)\n    \n    screen=screen.unsqueeze(0).to(device)\n    return screen\n\n\n\n\ndef optimize_model(memory,device,policy_net,target_net,optimizer,snake):\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n    \n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values = target_net(state_batch).max(1)[0].detach()\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    criterion = nn.MSELoss()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n    loss1 = round(loss.item(),3)\n    #print('loss = '+str(loss1))\n    \n    # Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()\n    \n    return loss1\n\n\n\n\n\nTransition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory():\n\n    def __init__(self, capacity):\n        self.memory = deque([],maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass DQN(nn.Module):\n\n    def __init__(self, inputs, outputs, device):\n        super(DQN, self).__init__()\n        self.head1 = nn.Linear(inputs, 512)\n        self.head2 = nn.Linear(512, 512)\n        self.head = nn.Linear(512, outputs)\n\n\n    def forward(self, x):\n        x = F.relu(self.head1(x))\n        x = F.relu(self.head2(x))\n        x = self.head(x)\n        #print(x)\n        return x\n    \n    \ndef create_agent(settings,device,MEMORY_SIZE):\n   \n    policy_net = DQN(100, n_actions,device).to(device)\n    target_net = DQN(100, n_actions,device).to(device)\n\n    \n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n    optimizer = optim.Adam(policy_net.parameters(), lr=1e-5)\n    memory = ReplayMemory(MEMORY_SIZE)\n    return policy_net, target_net, optimizer, memory\n\ndef select_action(state,policy_net,device):\n    with torch.no_grad():\n        w = policy_net(state)\n        w = w - w.min()\n        w = (w / w.sum())[0]\n        #w = w**3\n        #action = random.choices([0,1,2,3], weights=w, k=1)\n        action = np.argmax(w.cpu())\n        #action = torch.from_numpy(action).to(device)\n        action = torch.tensor(action, device=device)\n        action = action.view(1,1) \n        return action","metadata":{"collapsed":false,"_kg_hide-input":false,"jupyter":{"outputs_hidden":false}},"execution_count":null,"outputs":[]}]}